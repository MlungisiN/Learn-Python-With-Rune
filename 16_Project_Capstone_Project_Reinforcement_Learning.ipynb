{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MlungisiN/Learn-Python-With-Rune/blob/master/16_Project_Capstone_Project_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "848903f3",
      "metadata": {
        "id": "848903f3"
      },
      "source": [
        "<a \n",
        " href=\"https://colab.research.google.com/github/LearnPythonWithRune/LearnPython/blob/main/colab/starter/16 - Project - Capstone Project - Reinforcement Learning.ipynb\"\n",
        " target=\"_parent\">\n",
        "<img \n",
        " src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc01c381",
      "metadata": {
        "id": "fc01c381"
      },
      "source": [
        "# Capstone Project - Reinforcement Learning\n",
        "### Goal\n",
        "- Learn Object Oriented Programming techniques\n",
        "- Build your own Machine Learning model from scratch - Reinforcement Learning\n",
        "- Teach it to pickup and deliver items"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d385903a",
      "metadata": {
        "id": "d385903a"
      },
      "source": [
        "### What is Reinforcement Learning?\n",
        "- Reinforcement learning teaches the machine to think for itself based on past action rewards.\n",
        "![Reinforcement Learning](img/ReinforcementLearning.png)\n",
        "- Basically, the Reinforcement Learning algorithm tries to predict actions that gives rewards and avoids punishment.\n",
        "- It is like training a dog. You and the dog do not talk the same language, but the dogs learns how to act based on rewards (and punishment, which I do not advise or advocate).\n",
        "- Hence, if a dog is rewarded for a certain action in a given situation, then next time it is exposed to a similar situation it will act the same.\n",
        "- Translate that to Reinforcement Learning.\n",
        "    - The **agent** is the dog that is exposed to the **environment**.\n",
        "    - Then the **agent** encounters a **state**.\n",
        "    - The **agent** performs an **action** to transition to a **new state**.\n",
        "    - Then after the transition the **agent** receives a **reward** or **penalty (punishment)**.\n",
        "    - This forms a **policy** to create a strategy to choose actions in a given **state**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5bd04ad",
      "metadata": {
        "id": "f5bd04ad"
      },
      "source": [
        "### What algorithms are used for Reinforcement Learning?\n",
        "- The most common algorithm for Reinforcement Learning are.\n",
        "    - **Q-Learning**: is a model-free reinforcement learning algorithm to learn a policy telling an agent what action to take under what circumstances.\n",
        "    - **Temporal Difference**: refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function.\n",
        "    - **Deep Adversarial Network**: is a technique employed in the field of machine learning which attempts to fool models through malicious input.\n",
        "- We will focus on the **Q-learning** algorithm as it is easy to understand as well as powerful."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07ea59d7",
      "metadata": {
        "id": "07ea59d7"
      },
      "source": [
        "### How does the Q-learning algorithm work?\n",
        "- As already noted, I just love this algorithm. It is “easy” to understand and powerful as you will see.\n",
        "![Q-Learning algorithm](img/QLearning.png)\n",
        "- The **Q-Learning** algorithm has a **Q-table** (a Matrix of dimension state x actions – don’t worry if you do not understand what a Matrix is, you will not need the mathematical aspects of it – it is just an indexed “container” with numbers).\n",
        "- The **agent** (or **Q-Learning** algorithm) will be in a state.\n",
        "- Then in each iteration the **agent** needs take an **action**.\n",
        "- The **agent** will continuously update the **reward** in the **Q-table**.\n",
        "- The **learning** can come from either **exploiting** or **exploring**.\n",
        "- This translates into the following pseudo algorithm for the **Q-Learning**.\n",
        "- The **agent** is in a given **stateºº and needs to choose an **action**.\n",
        "\n",
        "#### Algorithm\n",
        "- Initialise the **Q-table** to all zeros\n",
        "- Iterate\n",
        "    - Agent is in state **state**.\n",
        "    - With probability **epsilon** choose to **explore**, else **exploit**.\n",
        "        - If **explore**, then choose a *random* **action**.\n",
        "        - If **exploit**, then choose the *best* **action** based on the current **Q-table**.\n",
        "    - Update the **Q-table** from the new **reward** to the previous state.\n",
        "    - Q[**state, action**] = (1 – **alpha**) * Q[**state, action**] + **alpha** * (**reward + gamma** * max(Q[**new_state**]) — Q[**state, action**])\n",
        "    \n",
        "#### Variables\n",
        "As you can se, we have introduced the following variables.\n",
        "\n",
        "- **epsilon**: the probability to take a random action, which is done to explore new territory.\n",
        "- **alpha**: is the learning rate that the algorithm should make in each iteration and should be in the interval from 0 to 1.\n",
        "- **gamma**: is the discount factor used to balance the immediate and future reward. This value is usually between 0.8 and 0.99\n",
        "- **reward**: is the feedback on the action and can be any number. Negative is penalty (or punishment) and positive is a reward."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec1392c1",
      "metadata": {
        "id": "ec1392c1"
      },
      "source": [
        "### Description of task we want to solve\n",
        "- To keep it simple, we create a field of size 10×10 positions. In that field there is an item that needs to be picked up and moved to a drop-off point.\n",
        "![Field](img/Field-ReinforcementLearning.png)\n",
        "- At each position there are 6 different actions that can be taken.\n",
        "    - **Action 0**: Go South if on field.\n",
        "    - **Action 1**: Go North if on field.\n",
        "    - **Action 2**: Go East if on field (Please notice, I mixed up East and West (East is Left here)).\n",
        "    - **Action 3**: Go West if on field (Please notice, I mixed up East and West (West is right here)).\n",
        "\n",
        "    - **Action 4**: Pickup item (it can try even if it is not there)\n",
        "    - **Action 5**: Drop-off item (it can try even if it does not have it)\n",
        "- Based on these actions we will make a reward system.\n",
        "    - If the **agent** tries to go off the **field**, punish with **-10** in **reward**.\n",
        "    - If the **agent** makes a (legal) move, punish with **-1** in **reward**, as we do not want to encourage endless walking around.\n",
        "    - If the **agent** tries to pick up item, but it is not there or it has it already, punish with **-10** in **reward**.\n",
        "    - If the **agent** picks up the item correct place, **reward** with **20**.\n",
        "    - If **agent** tries to drop-off item in wrong place or does not have the item, punish with **-10** in **reward**.\n",
        "    - If the **agent** drops-off item in correct place, **reward** with **20**.\n",
        "- That translates into the following code. I prefer to implement this code, as I think the standard libraries that provide similar frameworks hide some important details. As an example, and shown later, how do you map this into a state in the Q-table?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7824988a",
      "metadata": {
        "id": "7824988a"
      },
      "outputs": [],
      "source": [
        "class Field:\n",
        "    def __init__(self, size, item_pickup, item_drop_off, start_position):\n",
        "        self.size = size\n",
        "        self.item_pickup = item_pickup\n",
        "        self.item_drop_off = item_drop_off\n",
        "        self.position = start_position\n",
        "        self.item_in_car = False\n",
        "        \n",
        "    def get_number_of_states(self):\n",
        "        return self.size*self.size*self.size*self.size*2\n",
        "    \n",
        "    def get_state(self):\n",
        "        state = self.position[0]*self.size*self.size*self.size*2\n",
        "        state = state + self.position[1]*self.size*self.size*2\n",
        "        state = state + self.item_pickup[0]*self.size*2\n",
        "        state = state + self.item_pickup[1]*2\n",
        "        if self.item_in_car:\n",
        "            state = state + 1\n",
        "        return state\n",
        "        \n",
        "    def make_action(self, action):\n",
        "        (x, y) = self.position\n",
        "        if action == 0:  # Go South\n",
        "            if y == self.size - 1:\n",
        "                return -10, False\n",
        "            else:\n",
        "                self.position = (x, y + 1)\n",
        "                return -1, False\n",
        "        elif action == 1:  # Go North\n",
        "            if y == 0:\n",
        "                return -10, False\n",
        "            else:\n",
        "                self.position = (x, y - 1)\n",
        "                return -1, False\n",
        "        elif action == 2:  # Go East\n",
        "            if x == 0:\n",
        "                return -10, False\n",
        "            else:\n",
        "                self.position = (x - 1, y)\n",
        "                return -1, False\n",
        "        elif action == 3:  # Go West\n",
        "            if x == self.size - 1:\n",
        "                return -10, False\n",
        "            else:\n",
        "                self.position = (x + 1, y)\n",
        "                return -1, False\n",
        "        elif action == 4:  # Pickup item\n",
        "            if self.item_in_car:\n",
        "                return -10, False\n",
        "            elif self.item_pickup != (x, y):\n",
        "                return -10, False\n",
        "            else:\n",
        "                self.item_in_car = True\n",
        "                return 20, False\n",
        "        elif action == 5:  # Drop off item\n",
        "            if not self.item_in_car:\n",
        "                return -10, False\n",
        "            elif self.item_drop_off != (x, y):\n",
        "                self.item_pickup = (x, y)\n",
        "                self.item_in_car = False\n",
        "                return -10, False\n",
        "            else:\n",
        "                return 20, True\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7429203c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7429203c",
        "outputId": "c7a24787-a39d-409a-d2cc-f338c74e1e4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "field = Field(10, (0, 0), (9, 9), (9, 0))\n",
        "\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "field.make_action(2)\n",
        "\n",
        "field.make_action(4)\n",
        "\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "field.make_action(0)\n",
        "\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "field.make_action(3)\n",
        "\n",
        "field.make_action(5)\n",
        "\n",
        "field.position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8f18935b",
      "metadata": {
        "id": "8f18935b"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d53eb171",
      "metadata": {
        "id": "d53eb171"
      },
      "outputs": [],
      "source": [
        "def naive_solution():\n",
        "    size = 10\n",
        "    item_start = (0, 0)\n",
        "    item_drop_off = (9, 9)\n",
        "    start_position = (0, 9)\n",
        "    \n",
        "    field = Field(size, item_start, item_drop_off, start_position)\n",
        "    done = False\n",
        "    steps = 0\n",
        "    \n",
        "    while not done:\n",
        "        action = random.randint(0, 5)\n",
        "        reward, done = field.make_action(action)\n",
        "        steps = steps + 1\n",
        "    \n",
        "    return steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e547df33",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e547df33",
        "outputId": "1107f6f4-d8f0-4130-e7e6-d63cc0d10ab8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76856"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "naive_solution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bb8822c8",
      "metadata": {
        "id": "bb8822c8"
      },
      "outputs": [],
      "source": [
        "runs = [naive_solution() for _ in range(100)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1f952cb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f952cb1",
        "outputId": "df140eab-b383-4b4f-9465-193cef8e0586"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "185926.35"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "sum(runs)/len(runs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44a51674",
      "metadata": {
        "id": "44a51674"
      },
      "source": [
        "#### Algorithm\n",
        "- Initialise the **Q-table** to all zeros\n",
        "- Iterate\n",
        "    - Agent is in state **state**.\n",
        "    - With probability **epsilon** choose to **explore**, else **exploit**.\n",
        "        - If **explore**, then choose a *random* **action**.\n",
        "        - If **exploit**, then choose the *best* **action** based on the current **Q-table**.\n",
        "    - Update the **Q-table** from the new **reward** to the previous state.\n",
        "    - Q[**state, action**] = (1 – **alpha**) * Q[**state, action**] + **alpha** * (**reward + gamma** * max(Q[**new_state**]) — Q[**state, action**])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e9828fb6",
      "metadata": {
        "id": "e9828fb6"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b025b8e6",
      "metadata": {
        "id": "b025b8e6"
      },
      "outputs": [],
      "source": [
        "size = 10\n",
        "item_start = (0, 0)\n",
        "item_drop_off = (9, 9)\n",
        "start_position = (0, 9)\n",
        "\n",
        "field = Field(size, item_start, item_drop_off, start_position)\n",
        "\n",
        "number_of_states = field.get_number_of_states()\n",
        "number_of_actions = 6\n",
        "\n",
        "q_table = np.zeros((number_of_states, number_of_actions))\n",
        "\n",
        "epsilon = 0.1\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "\n",
        "for _ in range(10000):\n",
        "    field = Field(size, item_start, item_drop_off, start_position)\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        state = field.get_state()\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.randint(0, 5)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "            \n",
        "        reward, done = field.make_action(action)\n",
        "        # Q[state, action] = (1 – alpha) * Q[state, action] + alpha * (reward + gamma * max(Q[new_state]) — Q[state, action])\n",
        "        \n",
        "        new_state = field.get_state()\n",
        "        new_state_max = np.max(q_table[new_state])\n",
        "        \n",
        "        q_table[state, action] = (1 - alpha)*q_table[state, action] + alpha*(reward + gamma*new_state_max - q_table[state, action])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "03bffac3",
      "metadata": {
        "id": "03bffac3"
      },
      "outputs": [],
      "source": [
        "def reinforcement_learning():\n",
        "    epsilon = 0.1\n",
        "    alpha = 0.1\n",
        "    gamma = 0.6\n",
        "    \n",
        "    field = Field(size, item_start, item_drop_off, start_position)\n",
        "    done = False\n",
        "    steps = 0\n",
        "    \n",
        "    while not done:\n",
        "        state = field.get_state()\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.randint(0, 5)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "            \n",
        "        reward, done = field.make_action(action)\n",
        "        # Q[state, action] = (1 – alpha) * Q[state, action] + alpha * (reward + gamma * max(Q[new_state]) — Q[state, action])\n",
        "        \n",
        "        new_state = field.get_state()\n",
        "        new_state_max = np.max(q_table[new_state])\n",
        "        \n",
        "        q_table[state, action] = (1 - alpha)*q_table[state, action] + alpha*(reward + gamma*new_state_max - q_table[state, action])\n",
        "        \n",
        "        steps = steps + 1\n",
        "    \n",
        "    return steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fe01f9f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe01f9f7",
        "outputId": "22c04c0e-7b34-4ced-efd7-d8f90efe3803"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "reinforcement_learning()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f3808e31",
      "metadata": {
        "id": "f3808e31"
      },
      "outputs": [],
      "source": [
        "runs_rl = [reinforcement_learning() for _ in range(100)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b6be1fc2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6be1fc2",
        "outputId": "e2a53e08-8297-42eb-a244-cd30043cc5c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41.89"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "sum(runs_rl)/len(runs_rl)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f2Tw4df5VXl5"
      },
      "id": "f2Tw4df5VXl5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}